{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866c148f-ab67-4ee4-9a6e-7d53d4766623",
   "metadata": {},
   "source": [
    "# Data Scientist, Science and Natural Perlis Interview code by Isabel Smith "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7b675-a194-4dd4-822e-cd770d07a2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "import numpy as np\n",
    "from folium.plugins import HeatMap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62e917-fcef-440f-becd-1ecc92de7323",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Importing Data and Initial Analysis](#Importing-Data-and-Initial-Analysis)\n",
    "   1.1. [EM-DAT Data](#EM-DAT-Data)  \n",
    "   1.1.1 [Percentage of events per decade](#Percentage-of-events-per-decade)\n",
    "   1.1.2 [Exploring California](#Exploring-California)\n",
    "   1.2. [USGS Data](#USGS-Data)\n",
    "   1.2.1 [Percentage of Events](#Percentage-of-Events)\n",
    "\n",
    "2. [Comparison Interactive Figure](#Comparison-Interactive-Figure)\n",
    "\n",
    "3. [Exploring USGS Data Fully](#Exploring-USGS-Data-Fully)\n",
    "\n",
    "4. [Exploring EM-DAT Fully](#Exploring-EM-DAT-Fully)\n",
    "   4.1. [Variables in EM-DAT Over Time](#Variables-in-EM-DAT-Over-Time)\n",
    "   4.2. [Re-insurance Metrics](#Re-insurance-Metrics)\n",
    "\n",
    "5. [Comparison with Other Variables](#Comparison-with-Other-Variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367f610-56d5-4296-92ad-c58a5127b004",
   "metadata": {},
   "source": [
    "# 1.0 Importing Data plus inital analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76deed1b-8914-41af-a3f1-984b9f099e7c",
   "metadata": {},
   "source": [
    "## 1.1 EM-DAT Data\n",
    "\n",
    "The EM-DAT dataset contains information on disasters worldwide. In this section, we will load the dataset and perform some initial exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304605a-5c2f-4e54-b501-fb6e2c374501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the dataset\n",
    "df = pd.read_excel(\"/Users/isabelsmith/Documents/axa/EMDAT_Geophysical_Hydrological_Meteorological_North_American_Hazards_1900_2024[56].xlsx\")\n",
    "df_Geo = pd.DataFrame(df)\n",
    "\n",
    "# Filter earthquakes in the USA with magnitude >= 5\n",
    "df_quakes = df_Geo[(df_Geo['Disaster Type'] == 'Earthquake') &\n",
    "                   (df_Geo['Country'] == 'United States of America') &\n",
    "                   (df_Geo['Magnitude'] >= 5)]\n",
    "\n",
    "# Filter based on coordinates for Conterminous USA\n",
    "lat_min, lat_max = 24.6, 50.0\n",
    "lon_min, lon_max = -125.0, -65\n",
    "\n",
    "# Filter rows where Latitude and Longitude are within bounds\n",
    "df_quakes_Mex_included = df_quakes[(df_quakes['Latitude'] >= lat_min) & \n",
    "                                   (df_quakes['Latitude'] <= lat_max) &\n",
    "                                   (df_quakes['Longitude'] >= lon_min) & \n",
    "                                   (df_quakes['Longitude'] <= lon_max)]\n",
    "\n",
    "# Remove Mexico earthquake\n",
    "df_quakes = df_quakes_Mex_included[df_quakes_Mex_included['DisNo.'] != '1915-0004-USA']\n",
    "\n",
    "# Multiply the relevant columns by 1000 to convert '000 US$' to US$\n",
    "df_quakes['Insured Damage, Adjusted'] *= 1000\n",
    "df_quakes['Total Damage, Adjusted'] *= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c21257-c2bb-4f5a-af35-48163a8e1aa8",
   "metadata": {},
   "source": [
    "## 1.1.1 Percentage of events per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3653d0a-1e25-4480-a2cc-ee0f1c60c582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_quakes['Year'] = df_quakes['DisNo.'].str.split('-').str[0].astype(int)\n",
    "df_quakes['Decade'] = (df_quakes['Year'] // 10) * 10\n",
    "decade_counts = df_quakes['Decade'].value_counts().sort_index()\n",
    "\n",
    "# Step 1: Calculate percentage of events per decade\n",
    "total_events = len(df_quakes)\n",
    "decade_percentages = (decade_counts / total_events) * 100\n",
    "\n",
    "# Step 2: Combine the counts and percentages into a single DataFrame\n",
    "decade_summary = pd.DataFrame({\n",
    "    'Event Count': decade_counts,\n",
    "    'Percentage of Total Events': decade_percentages\n",
    "})\n",
    "decade_summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b1ec9-92ff-4bb3-bbcb-f119ff2af97e",
   "metadata": {},
   "source": [
    "## 1.1.2 Exploring Califonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3b5df-9d98-4c21-a999-b0573d8c5570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the latitude and longitude boundaries for California (sourced from https://www.mapsofworld.com/usa/states/california/lat-long.html)\n",
    "lat_min, lat_max = 32.5, 42.0\n",
    "lon_min, lon_max = -124.5, -113.5\n",
    "\n",
    "# Filtering for earthquakes that occurred in California\n",
    "california_quakes = df_quakes[(df_quakes['Latitude'] >= lat_min) & \n",
    "                              (df_quakes['Latitude'] <= lat_max) & \n",
    "                              (df_quakes['Longitude'] >= lon_min) & \n",
    "                              (df_quakes['Longitude'] <= lon_max)]\n",
    "\n",
    "# Counting the number of earthquakes in California\n",
    "num_events_california = california_quakes.shape[0]\n",
    "\n",
    "# Counting the total number of earthquakes in the entire dataset\n",
    "total_events = df_quakes.shape[0]\n",
    "\n",
    "# Calculate the percentage of events in California\n",
    "percentage_california = (num_events_california / total_events) * 100\n",
    "\n",
    "print(f\"Number of earthquake events in California: {num_events_california}\")\n",
    "print(f\"Percentage of earthquake events in California: {percentage_california:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a58b61-10ce-4234-9d5d-ba156a27fdbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 USGS Data\n",
    "\n",
    "The USGS dataset contains geological data related to earthquakes and other natural hazards. We will load and analyze this data as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95377e-4411-4571-9f6c-e833528738ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_and_filter_emdat_geophysical_data(file_path):\n",
    "    \"\"\"\n",
    "    Uploading the geophysical hazards dataset. Despite downloading the contiguous US region, \n",
    "    several points developed over Mexico and Canada. This function removes them to focus on the US.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing cleaned geophysical data points.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df_data = pd.read_excel(file_path)\n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # Reverse the dataset for easier comparison, swapping to start at 1900\n",
    "    df = df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    # Part 1: Filter to exclude points outside the contiguous US\n",
    "    def filter_us_points(df, lat_min=24.6, lat_max=50.0, lon_min=-125.0, lon_max=-65.0):\n",
    "        # Bounding box filter\n",
    "        in_box = (\n",
    "            (df['Latitude'] >= lat_min) & (df['Latitude'] <= lat_max) &\n",
    "            (df['Longitude'] >= lon_min) & (df['Longitude'] <= lon_max)\n",
    "        )\n",
    "        \n",
    "        # Exclude Canada and Mexico\n",
    "        not_canada = df['Latitude'] < 49.0  # Approximation for Canada\n",
    "        not_mexico = ~((df['Latitude'] < 32.0) & (df['Longitude'] > -117.0))  # Approximation for Mexico\n",
    "        \n",
    "        # Combine filters\n",
    "        is_us = in_box & not_canada & not_mexico\n",
    "        \n",
    "        # Return filtered DataFrame\n",
    "        return df[is_us]\n",
    "\n",
    "    df = filter_us_points(df)\n",
    "\n",
    "    # Part 2: Further removal of points outside specific excluded regions\n",
    "    exclusions = [\n",
    "        # 1. Ottawa-Sudbury-Peterborough region\n",
    "        (44.0, 47.5, -83.5, -75.0),\n",
    "        # 2. South of Tijuana to San Felipe\n",
    "        (30.0, 32.5, -116.5, -114.0),\n",
    "        # 3. West into the Pacific past Guadalupe\n",
    "        (28.0, 33.0, -120.0, -116.5),\n",
    "        # 4. East to Puerto PeÃ±asco\n",
    "        (30.5, 32.5, -114.5, -112.0),\n",
    "        # 5. West of Quebec\n",
    "        (46, 49, -72, -30.0)\n",
    "    ]\n",
    "\n",
    "    for lat_min, lat_max, lon_min, lon_max in exclusions:\n",
    "        df = df[\n",
    "            ~(\n",
    "                (df['Latitude'] >= lat_min) & (df['Latitude'] <= lat_max) &\n",
    "                (df['Longitude'] >= lon_min) & (df['Longitude'] <= lon_max)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Part 3: Remove rows with specific 'DisNo.' values\n",
    "    times_to_exclude = [\n",
    "        '1979-10-15T23:16:53.910Z',\n",
    "        '1951-01-24T07:16:52.620Z',\n",
    "        '1976-05-16T08:35:14.800Z'  # Added this time\n",
    "    ]\n",
    "    df = df[~df['time'].isin(times_to_exclude)]\n",
    "\n",
    "    return df\n",
    "\n",
    "file_path = \"/Users/isabelsmith/Documents/axa/Axa_part2_data.xlsx\"\n",
    "df_USGS = process_and_filter_emdat_geophysical_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b260dc9-d841-43c1-91c8-ece57b343c7f",
   "metadata": {},
   "source": [
    "## 1.2.1 Percentage of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81f021-8776-48c9-badf-d396a54fc004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_USGS['Year'] = df_USGS['time'].str.split('-').str[0].astype(int)\n",
    "df_USGS['Decade'] = (df_USGS['Year'] // 10) * 10\n",
    "decade_counts = df_USGS['Decade'].value_counts().sort_index()\n",
    "\n",
    "# Step 1: Calculate percentage of events per decade\n",
    "total_events = len(df_USGS)\n",
    "decade_percentages = (decade_counts / total_events) * 100\n",
    "\n",
    "# Step 2: Combine the counts and percentages into a single DataFrame\n",
    "decade_summary_USGS = pd.DataFrame({\n",
    "    'Event Count': decade_counts,\n",
    "    'Percentage of Total Events': decade_percentages\n",
    "})\n",
    "decade_summary_USGS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf72792-42ef-4fdd-9102-209575e78d50",
   "metadata": {},
   "source": [
    "# 2.0 Comparison interactive figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b46bed-e7a7-4dbc-85de-f7c38ea1254f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_combined_earthquake_map_with_time_and_magnitude(df_quakes, df_USGS):\n",
    "    \"\"\"\n",
    "    Here I have created an interactive map plotting earthquakes from df_quakes in blue, df_USGS in red,\n",
    "    and overlapping points in black. Displays 'Time' and 'Magnitude' once clicked on.\n",
    "\n",
    "    Parameters:\n",
    "        df_quakes (DataFrame): DataFrame containing earthquake data with 'Latitude', 'Longitude',\n",
    "                               'mag' (magnitude), and 'DisNo.' (time).\n",
    "        df_USGS (DataFrame): DataFrame containing earthquake data with 'Latitude', 'Longitude',\n",
    "                             'Magnitude', and 'Time'.\n",
    "\n",
    "    Returns:\n",
    "        df_sub_1 (DataFrame): Rows from df_USGS that overlap with df_quakes.\n",
    "        df_sub_2 (DataFrame): Rows from df_quakes that overlap with df_USGS.\n",
    "    \"\"\"\n",
    "    # Extract latitude, longitude, magnitude, and time from both DataFrames\n",
    "    lat_quakes = df_quakes['Latitude']\n",
    "    lon_quakes = df_quakes['Longitude']\n",
    "    mag_quakes = df_quakes['Magnitude']  # Magnitude in df_quakes\n",
    "    time_quakes = df_quakes['DisNo.']  # Time in df_quakes\n",
    "\n",
    "    lat_usgs = df_USGS['Latitude']\n",
    "    lon_usgs = df_USGS['Longitude']\n",
    "    mag_usgs = df_USGS['mag']\n",
    "    time_usgs = df_USGS['time']\n",
    "\n",
    "    # Define a function to check proximity\n",
    "    def is_close(lat1, lon1, lat2, lon2, tolerance=0.01):\n",
    "        return abs(lat1 - lat2) <= tolerance and abs(lon1 - lon2) <= tolerance\n",
    "\n",
    "    # Find overlapping points\n",
    "    overlap_indices_usgs = []\n",
    "    overlap_indices_quakes = []\n",
    "    for i, (lat_usgs_val, lon_usgs_val) in enumerate(zip(lat_usgs, lon_usgs)):\n",
    "        for j, (lat_quakes_val, lon_quakes_val) in enumerate(zip(lat_quakes, lon_quakes)):\n",
    "            if is_close(lat_usgs_val, lon_usgs_val, lat_quakes_val, lon_quakes_val):\n",
    "                overlap_indices_usgs.append(i)  # USGS index\n",
    "                overlap_indices_quakes.append(j)  # Quakes index\n",
    "                break  # Stop once an overlap is found\n",
    "\n",
    "    # Create subsets for overlapping points\n",
    "    df_sub_1 = df_USGS.iloc[overlap_indices_usgs].reset_index(drop=True)\n",
    "    df_sub_2 = df_quakes.iloc[overlap_indices_quakes].reset_index(drop=True)\n",
    "\n",
    "    # Initialize the map at the center of the two datasets\n",
    "    center_lat = (lat_quakes.mean() + lat_usgs.mean()) / 2\n",
    "    center_lon = (lon_quakes.mean() + lon_usgs.mean()) / 2\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=5)\n",
    "\n",
    "    # Add points from df_quakes in blue with time and magnitude\n",
    "    for lat, lon, mag, time in zip(lat_quakes, lon_quakes, mag_quakes, time_quakes):\n",
    "        tooltip_content = f\"<strong>Time (DisNo.):</strong> {time}<br><strong>Magnitude:</strong> {mag}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=4,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            tooltip=tooltip_content\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add points from df_USGS in red with time and magnitude\n",
    "    for lat, lon, mag, time in zip(lat_usgs, lon_usgs, mag_usgs, time_usgs):\n",
    "        tooltip_content = f\"<strong>Time:</strong> {time}<br><strong>Magnitude:</strong> {mag}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=4,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            tooltip=tooltip_content\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add overlapping points in black with time and magnitude\n",
    "    for lat, lon, mag, time in zip(df_sub_1['Latitude'], df_sub_1['Longitude'], df_sub_1['mag'], df_sub_1['time']):\n",
    "        tooltip_content = f\"<strong>Time:</strong> {time}<br><strong>Magnitude:</strong> {mag}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=6,  # Slightly larger radius to highlight\n",
    "            color='black',\n",
    "            fill=True,\n",
    "            fill_opacity=1.0,\n",
    "            tooltip=tooltip_content\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Save the map\n",
    "    m.save('earthquake_map_with_time_and_magnitude.html')\n",
    "\n",
    "    return df_sub_1, df_sub_2\n",
    "\n",
    "df_sub_1, df_sub_2 = create_combined_earthquake_map_with_time_and_magnitude(df_quakes, df_USGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204d2b8-2ad1-40ea-836b-877682b8f75c",
   "metadata": {},
   "source": [
    "# 3.0 Exploring USGS Data fully "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9459d82-59a5-4a9f-8236-b3fdcfef16a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_earthquake_data_split_filled_scaled_combined(df_USGS):\n",
    "    \"\"\"\n",
    "    Creating two plots to explore USGS over time. \n",
    "    - The first plot shows 'mag', 'not', and 'dmin'.\n",
    "    - The second plot shows 'rms', 'depth', and 'gap'.\n",
    "    \n",
    "    Parameters:\n",
    "        df_USGS (DataFrame): DataFrame with earthquake data containing \n",
    "                             'time', 'mag', 'not', 'dmin', 'rms', 'depth', and 'gap'.\n",
    "    \"\"\"\n",
    "    # Ensure the Date column exists and is properly formatted\n",
    "    if 'Date' not in df_USGS.columns:\n",
    "        df_USGS['Date'] = pd.to_datetime(df_USGS['time'])  # Convert 'time' column to datetime\n",
    "\n",
    "    # Fill missing values in numeric columns\n",
    "    numeric_cols = df_USGS.select_dtypes(include=[np.number]).columns\n",
    "    df_USGS[numeric_cols] = df_USGS[numeric_cols].interpolate(method='linear')\n",
    "\n",
    "    # Variables for plotting\n",
    "    variables_subplot1 = {\n",
    "        \"mag\": {\"color\": \"blue\", \"linestyle\": \"-\", \"alpha\": 0.5},\n",
    "        \"nst\": {\"color\": \"orange\", \"linestyle\": \"--\", \"alpha\": 0.5},\n",
    "        \"dmin\": {\"color\": \"green\", \"linestyle\": \"-.\", \"alpha\": 0.5},\n",
    "    }\n",
    "    variables_subplot2 = {\n",
    "        \"rms\": {\"color\": \"red\", \"linestyle\": \"-\", \"alpha\": 0.5},\n",
    "        \"depth\": {\"color\": \"blue\", \"linestyle\": \"--\", \"alpha\": 0.5},\n",
    "        \"gap\": {\"color\": \"grey\", \"linestyle\": \"-.\", \"alpha\": 0.5},\n",
    "    }\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Function to plot line graphs with independent y-axes and filled regions\n",
    "    def plot_filled_lines(ax, x, y, label, color, linestyle, alpha):\n",
    "        line, = ax.plot(x, y, color=color, linestyle=linestyle, label=label)\n",
    "        ax.fill_between(x, 0, y, color=color, alpha=alpha)\n",
    "        ax.set_ylabel(label, color=color)\n",
    "        ax.tick_params(axis='y', labelcolor=color)\n",
    "        return line\n",
    "\n",
    "    # Plot Subplot 1\n",
    "    offset = 0\n",
    "    for var, props in variables_subplot1.items():\n",
    "        ax = axes[0].twinx() if offset > 0 else axes[0] \n",
    "        plot_filled_lines(ax, df_USGS['Date'], df_USGS[var], var, \n",
    "                          props[\"color\"], props[\"linestyle\"], props[\"alpha\"])\n",
    "        if offset > 0:  # Adjust position of spines for additional y-axes\n",
    "            ax.spines[\"right\"].set_position((\"outward\", 60 * offset))\n",
    "        offset += 1  # Increment offset for each new variable\n",
    "\n",
    "    axes[0].set_title(\"The magnitude for the event (Mag), Number of seismic stations (nst), Distance from the epicenter (dmin).\")\n",
    "    axes[0].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Plot Subplot 2\n",
    "    offset = 0\n",
    "    for var, props in variables_subplot2.items():\n",
    "        ax = axes[1].twinx() if offset > 0 else axes[1] \n",
    "        plot_filled_lines(ax, df_USGS['Date'], df_USGS[var], var, \n",
    "                          props[\"color\"], props[\"linestyle\"], props[\"alpha\"])\n",
    "        if offset > 0:  # Adjust position of spines for additional y-axes\n",
    "            ax.spines[\"right\"].set_position((\"outward\", 60 * offset))\n",
    "        offset += 1  # Increment offset for each new variable\n",
    "\n",
    "    axes[1].set_title(\"Time residual (rms), Depth of rupture (Depth), Gap between stations (Gap)\")\n",
    "    axes[1].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Format x-axis for clarity\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())  # Auto-tick based on date range\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Shared x-axis label\n",
    "    axes[1].set_xlabel(\"Date\")\n",
    "\n",
    "    # Tighten layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_earthquake_data_split_filled_scaled_combined(df_USGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd4d3bb-557f-48c6-9d2c-ecd7eb250c3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4.0 Exploring EM-DATA fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde91078-171f-4757-a808-a24cfb34fb09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_earthquake_map(df_quakes):\n",
    "    \"\"\"\n",
    "    Another interactive map of earthquakes but just for EM_DAT and exploring more detals such as Magnitude, Total Deaths,\n",
    "    Total Damage (Adjusted), Number of Injured, and Disaster ID. \n",
    "    The colour scale represents earthquake magnitude.\n",
    "\n",
    "    Parameters:\n",
    "        df_quakes (DataFrame): A pandas DataFrame with columns 'Latitude', 'Longitude', 'Magnitude',\n",
    "                               'Total Deaths', 'Total Damage, Adjusted', 'No. Injured', and 'DisNo.'.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves the map as an HTML file ('earthquake_map_end.html').\n",
    "    \"\"\"\n",
    "    # Extract data for mapping\n",
    "    Lat = df_quakes['Latitude']\n",
    "    Lon = df_quakes['Longitude']\n",
    "    Mag = df_quakes['Magnitude']\n",
    "    Deaths = df_quakes['Total Deaths']\n",
    "    Injured = df_quakes['No. Injured']\n",
    "    Insured_Damage = df_quakes['Insured Damage, Adjusted']\n",
    "    DisNo = df_quakes['DisNo.']  # Year/ID\n",
    "\n",
    "    # Create a map centered around the mean coordinates\n",
    "    m = folium.Map(location=[Lat.mean(), Lon.mean()], zoom_start=5)\n",
    "\n",
    "    # Define a color scale for the Magnitude using 'viridis'\n",
    "    min_mag = Mag.min()\n",
    "    max_mag = Mag.max()\n",
    "    \n",
    "    # Create a Normalize object to map the magnitude to a color\n",
    "    norm = mcolors.Normalize(vmin=min_mag, vmax=max_mag)\n",
    "    cmap = plt.cm.get_cmap('viridis_r')  # 'viridis' is one of the color maps in Matplotlib\n",
    "\n",
    "    # Add earthquake data points to the map\n",
    "    for lat, lon, mag, deaths, damage, injured, insured_damage, disno in zip(Lat, Lon, Mag, Deaths, Damage, Injured, Insured_Damage, DisNo):\n",
    "        # Normalize the magnitude for color scaling\n",
    "        norm_mag = norm(mag)  # Normalized magnitude value\n",
    "        \n",
    "        # Get the color based on the normalized magnitude\n",
    "        color = mcolors.to_hex(cmap(norm_mag))  # Convert to hex color code\n",
    "        \n",
    "        # Handle NaN values in each column - display \"0\" in the tooltip if missing\n",
    "        deaths_text = \"0\" if np.isnan(deaths) else int(deaths)\n",
    "        damage_text = \"0\" if np.isnan(damage) else int(damage)\n",
    "        injured_text = \"0\" if np.isnan(injured) else int(injured)\n",
    "        insured_damage_text = \"0\" if np.isnan(insured_damage) else int(insured_damage)\n",
    "        \n",
    "        # Create tooltip content\n",
    "        tooltip_content = f\"\"\"\n",
    "        <strong>Disaster ID:</strong> {disno}<br>\n",
    "        <strong>Magnitude:</strong> {mag}<br>\n",
    "        <strong>Total Deaths:</strong> {deaths_text}<br>\n",
    "        <strong>Number Injured:</strong> {injured_text}<br>\n",
    "        <strong>Total Damage (Adjusted) US$):</strong> {damage_text}<br>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a circle marker with the magnitude-based color scale\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=mag,  # Circle size proportional to magnitude\n",
    "            color=color,  # Apply color based on magnitude\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            tooltip=folium.Tooltip(tooltip_content)  # Add detailed tooltip\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Save the map as an HTML file\n",
    "    m.save('earthquake_map_end.html')\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df_quakes contains the appropriate columns:\n",
    "create_earthquake_map(df_quakes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08cf10f-c5fa-4997-98b6-548198f546c4",
   "metadata": {},
   "source": [
    "## 4.1 Variables in EM-DAT over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dde1be-9725-4a96-9dc0-eccda547f4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_earthquake_data_split_filled_scaled_combined(df_quakes):\n",
    "    \"\"\"\n",
    "    This function creates two subplots with primary y-axes showing Total Deaths and Insured Damage.\n",
    "    \n",
    "    Parameters:\n",
    "        df_quakes (DataFrame): DataFrame with 'DisNo.', 'Total Deaths', \n",
    "                               'No. Injured', 'No. Affected', 'CPI', \n",
    "                               'Insured Damage, Adjusted', 'Total Damage, Adjusted'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the year from 'DisNo.'\n",
    "    df_quakes['Year'] = df_quakes['DisNo.'].str[:4].astype(int)\n",
    "    df_quakes['Date'] = pd.to_datetime(df_quakes['Year'].astype(str), format='%Y')\n",
    "\n",
    "    # Fill missing values in numeric columns\n",
    "    numeric_cols = df_quakes.select_dtypes(include=[np.number]).columns\n",
    "    df_quakes[numeric_cols] = df_quakes[numeric_cols].interpolate(method='linear')\n",
    "\n",
    "    # Variables for plotting\n",
    "    variables_subplot1 = {\n",
    "        \"No. Affected\": {\"color\": \"grey\", \"linestyle\": \"-\", \"alpha\": 0.5},\n",
    "        \"No. Injured\": {\"color\": \"orange\", \"linestyle\": \"-.\", \"alpha\": 0.5},\n",
    "    }\n",
    "    variables_subplot2 = {\n",
    "        \"CPI\": {\"color\": \"orange\", \"linestyle\": \"-\", \"alpha\": 0.5},\n",
    "        # Total Damage is already handled as a single axis, no need to repeat here\n",
    "    }\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Function to plot line graphs with independent y-axes and filled regions\n",
    "    def plot_filled_lines(ax, x, y, label, color, linestyle, alpha):\n",
    "        line, = ax.plot(x, y, color=color, linestyle=linestyle, label=label)\n",
    "        ax.fill_between(x, 0, y, color=color, alpha=alpha)\n",
    "        ax.set_ylabel(label, color=color)\n",
    "        ax.tick_params(axis='y', labelcolor=color)\n",
    "        return line\n",
    "\n",
    "    # Plot Subplot 1\n",
    "    # Primary y-axis (Total Deaths)\n",
    "    axes[0].plot(df_quakes['Date'], df_quakes['Total Deaths'], color=\"blue\", linestyle=\"-\", label=\"Total Deaths\")\n",
    "    axes[0].fill_between(df_quakes['Date'], 0, df_quakes['Total Deaths'], color=\"blue\", alpha=0.5)\n",
    "    axes[0].set_ylabel(\"Total Deaths\", color=\"blue\")\n",
    "    axes[0].tick_params(axis='y', labelcolor=\"blue\")\n",
    "\n",
    "    # Secondary y-axes (No. Affected, No. Injured) with adjustments to avoid overlap\n",
    "    offset = 50\n",
    "    for var, props in variables_subplot1.items():\n",
    "        ax = axes[0].twinx()  # Create independent y-axis\n",
    "        plot_filled_lines(ax, df_quakes['Date'], df_quakes[var], var, \n",
    "                          props[\"color\"], props[\"linestyle\"], props[\"alpha\"])\n",
    "        ax.spines[\"right\"].set_position((\"outward\", offset))  # Adjust the position to prevent overlap\n",
    "        offset += 50  # Increment offset for the next y-axis\n",
    "\n",
    "    axes[0].set_title(\"Earthquake Impacts: Deaths, Affected, and Injured\")\n",
    "    axes[0].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Plot Subplot 2\n",
    "    # Primary y-axis (Insured Damage, Adjusted)\n",
    "    axes[1].plot(df_quakes['Date'], df_quakes['Insured Damage, Adjusted'], color=\"black\", linestyle=\"-\", label=\"Insured Damage, Adjusted\")\n",
    "    axes[1].fill_between(df_quakes['Date'], 0, df_quakes['Insured Damage, Adjusted'], color=\"black\", alpha=0.5)\n",
    "    axes[1].set_ylabel(\"Insured Damage (US$)\", color=\"black\")\n",
    "    axes[1].tick_params(axis='y', labelcolor=\"black\")\n",
    "\n",
    "    # Secondary y-axes (CPI)\n",
    "    for var, props in variables_subplot2.items():\n",
    "        ax = axes[1].twinx()  # Create independent y-axis\n",
    "        plot_filled_lines(ax, df_quakes['Date'], df_quakes[var], var, \n",
    "                          props[\"color\"], props[\"linestyle\"], props[\"alpha\"])\n",
    "\n",
    "    # Label Total Damage axis explicitly (only one axis for Total Damage)\n",
    "    ax_total_damage = axes[1].twinx()  # Use the same independent y-axis for Total Damage\n",
    "    ax_total_damage.spines[\"right\"].set_position((\"outward\", 60))  # Move it a bit to the right for clarity\n",
    "    ax_total_damage.plot(df_quakes['Date'], df_quakes['Total Damage, Adjusted'], color=\"green\", linestyle=\"-.\", label=\"Total Damage, Adjusted\")\n",
    "    ax_total_damage.fill_between(df_quakes['Date'], 0, df_quakes['Total Damage, Adjusted'], color=\"green\", alpha=0.5)\n",
    "    ax_total_damage.set_ylabel(\"Total Damage (US$)\", color=\"green\")\n",
    "    ax_total_damage.tick_params(axis='y', labelcolor=\"green\")\n",
    "\n",
    "    axes[1].set_title(\"Economic Impacts: Insured Damage, CPI, and Total Damage\")\n",
    "    axes[1].grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # Format x-axis for clarity\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator(5))  # Major ticks every 5 years\n",
    "\n",
    "    # Shared x-axis label\n",
    "    axes[1].set_xlabel(\"Year\")\n",
    "\n",
    "    # Tighten layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_earthquake_data_split_filled_scaled_combined(df_quakes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa95f71-f975-4ff8-955e-468018048700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### Creating a table of averages within new magnitude range #######\n",
    "\n",
    "def assign_magnitude_range(magnitude):\n",
    "    '''Here we defined the magnitude ranges'''\n",
    "    if 5.0 <= magnitude < 6.0:\n",
    "        return \"5.0-5.9\"\n",
    "    elif 6.0 <= magnitude < 7.0:\n",
    "        return \"6.0-6.9\"\n",
    "    elif 7.0 <= magnitude < 8.0:\n",
    "        return \"7.0-7.9\"\n",
    "    else:\n",
    "        return \"8.0+\"\n",
    "\n",
    "\n",
    "# Apply the magnitude range assignment\n",
    "df_quakes['Magnitude'] = df_quakes['Magnitude'].apply(assign_magnitude_range)\n",
    "\n",
    "# Select the variables of interest, excluding 'Excess Loss' and 'Reinsured Loss' for now\n",
    "variables_of_interest = [\n",
    "    'Magnitude', 'Total Deaths', 'No. Injured', \n",
    "    'No. Affected', 'Reconstruction Costs, Adjusted', 'Insured Damage, Adjusted', \n",
    "    'Total Damage, Adjusted', 'CPI', 'Start Year'\n",
    "]\n",
    "df_quakes_selected = df_quakes[variables_of_interest]\n",
    "\n",
    "# Group by magnitude range and calculate the median for each column\n",
    "grouped_ranges = df_quakes_selected.groupby('Magnitude').median(numeric_only=True).reset_index()\n",
    "\n",
    "# Step 1: Categorize the quakes into decades\n",
    "df_quakes_selected['Decade'] = (df_quakes_selected['Start Year'] // 10) * 10\n",
    "\n",
    "# Step 2: Count the number of events in each decade\n",
    "event_counts_by_decade = df_quakes_selected.groupby('Decade').size()\n",
    "\n",
    "# Step 3: Calculate the probability per decade\n",
    "decade_probabilities = event_counts_by_decade / 10  # Each decade has 10 years\n",
    "\n",
    "# Step 4: Calculate weighted probability of each magnitude based on decade probabilities\n",
    "total_years = df_quakes_selected['Start Year'].nunique()  # Total number of unique years\n",
    "weighted_probabilities = []\n",
    "\n",
    "for _, row in grouped_ranges.iterrows():\n",
    "    # Extract the years for this magnitude range\n",
    "    magnitude_range = row['Magnitude']\n",
    "    years_in_magnitude = df_quakes_selected[df_quakes_selected['Magnitude'] == magnitude_range]['Start Year']\n",
    "    \n",
    "    # Get the unique decades this magnitude range appears in\n",
    "    unique_decades = years_in_magnitude // 10 * 10\n",
    "    \n",
    "    # Calculate the weighted average probability for this magnitude\n",
    "    weighted_probability = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for decade in unique_decades:\n",
    "        # The weight for each decade is the number of years this magnitude appears in that decade\n",
    "        years_in_decade = years_in_magnitude[years_in_magnitude // 10 * 10 == decade].nunique()\n",
    "        weight = years_in_decade / 10  # Normalize by number of years in the decade\n",
    "        \n",
    "        # Add the weighted probability\n",
    "        weighted_probability += (decade_probabilities.get(decade, 0) * weight)\n",
    "        total_weight += weight\n",
    "\n",
    "    # Normalize the weighted probability\n",
    "    if total_weight > 0:\n",
    "        weighted_probabilities.append(weighted_probability / total_weight)\n",
    "    else:\n",
    "        weighted_probabilities.append(0)\n",
    "\n",
    "# Step 5: Add the calculated probabilities to the grouped results\n",
    "grouped_ranges['Probability'] = weighted_probabilities\n",
    "\n",
    "print(grouped_ranges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558a6a1-2e32-4637-aa49-414475f7458a",
   "metadata": {},
   "source": [
    "## 4.2 Re-isurance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862d469-0a57-47b1-ac2b-8f38047d2e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_weighted_probabilities(df_quakes, time_column='Start Year', magnitude_column='Magnitude'):\n",
    "    \"\"\"\n",
    "    This function calculates a probability distribution for each magnitude based on weighted frequency \n",
    "    considering the irregular time series data (time gaps).\n",
    "    \"\"\"\n",
    "    # Sort the dataframe by the time column to ensure events are in order\n",
    "    df_quakes_sorted = df_quakes.sort_values(by=time_column)\n",
    "    \n",
    "    # Calculate the time differences between each earthquake event\n",
    "    df_quakes_sorted['Time Gap'] = df_quakes_sorted[time_column].diff().fillna(0)\n",
    "    \n",
    "    # Calculate the weighted frequency for each magnitude\n",
    "    # The weight is inversely proportional to the time gap: smaller gaps (more frequent events) get higher weights\n",
    "    df_quakes_sorted['Weight'] = 1 / (df_quakes_sorted['Time Gap'] + 1e-6)  # Adding a small epsilon to avoid division by zero\n",
    "    \n",
    "    # Group by magnitude and sum the weights to get the weighted frequency\n",
    "    weighted_frequencies = df_quakes_sorted.groupby(magnitude_column)['Weight'].sum().reset_index()\n",
    "    \n",
    "    # Normalize the weighted frequencies to get probabilities\n",
    "    total_weight = weighted_frequencies['Weight'].sum()\n",
    "    weighted_frequencies['Probability'] = weighted_frequencies['Weight'] / total_weight\n",
    "    \n",
    "    return weighted_frequencies[['Magnitude', 'Probability']]\n",
    "\n",
    "def calculate_reinsurance_coverage_with_weighted_probabilities(df_quakes, premium_rate=0.10, attachment_point=50000000):\n",
    "    \"\"\"\n",
    "    This function calculates various reinsurance-related metrics based on earthquake data:\n",
    "    - Expected Loss\n",
    "    - Excess Loss\n",
    "    - Reinsurance Premium\n",
    "    - Net Loss After Reinsurance\n",
    "    - Final Adjusted Loss\n",
    "    The probabilities are calculated using the weighted frequency of magnitudes in the dataset.\n",
    "    \"\"\"\n",
    "    # Ensure the necessary columns exist\n",
    "    required_columns = ['Magnitude', 'Insured Damage, Adjusted', 'Start Year']\n",
    "    if not all(col in df_quakes.columns for col in required_columns):\n",
    "        raise ValueError(\"DataFrame must contain 'Magnitude', 'Insured Damage, Adjusted', and 'Start Year' columns.\")\n",
    "\n",
    "    # Step 1: Calculate weighted probabilities based on the magnitudes\n",
    "    weighted_probabilities = calculate_weighted_probabilities(df_quakes)\n",
    "\n",
    "    # Step 2: Merge the weighted probabilities back into df_quakes\n",
    "    df_quakes = df_quakes.merge(weighted_probabilities, on='Magnitude', how='left')\n",
    "\n",
    "    # Step 3: Calculate Expected Loss (Insured Damage * Probability of Occurrence), accounting for '000 US $' units\n",
    "    df_quakes['Expected Loss'] = (df_quakes['Insured Damage, Adjusted']) * df_quakes['Probability']\n",
    "\n",
    "    # Step 4: Calculate Excess Loss (If Total Loss Exceeds the Attachment Point)\n",
    "    df_quakes['Excess Loss'] = (df_quakes['Insured Damage, Adjusted']) - attachment_point\n",
    "    df_quakes['Excess Loss'] = df_quakes['Excess Loss'].apply(lambda x: max(0, x))  # Only apply excess if > 0\n",
    "\n",
    "    # Step 5: Calculate Reinsurance Premium (Expected Loss * Premium Rate)\n",
    "    df_quakes['Reinsurance Premium'] = df_quakes['Expected Loss'] * premium_rate\n",
    "\n",
    "    # Step 6: Calculate Net Loss After Reinsurance (Total Insured Damage - Excess Loss Covered by Reinsurer)\n",
    "    df_quakes['Net Loss After Reinsurance'] = (df_quakes['Insured Damage, Adjusted']) - df_quakes['Excess Loss']\n",
    "\n",
    "    # Step 7: Calculate Final Adjusted Loss (Net Loss After Reinsurance - Reinsurance Premium)\n",
    "    df_quakes['Final Adjusted Loss'] = df_quakes['Net Loss After Reinsurance'] - df_quakes['Reinsurance Premium']\n",
    "\n",
    "    # Return the updated DataFrame with all the calculated columns\n",
    "    return df_quakes[['Magnitude', 'Insured Damage, Adjusted', 'Probability', 'Expected Loss', \n",
    "                      'Excess Loss', 'Reinsurance Premium', 'Net Loss After Reinsurance', \n",
    "                      'Final Adjusted Loss']]\n",
    "\n",
    "df_quakes_updated = calculate_reinsurance_coverage_with_weighted_probabilities(df_quakes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf9033-d220-4fe4-b316-0f01ba5b9b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Features to plot\n",
    "features = ['Expected Loss', 'Excess Loss', 'Final Adjusted Loss', 'Reinsurance Premium']\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(features):\n",
    "    scatter = sns.scatterplot(\n",
    "        data=df_quakes_updated,\n",
    "        x='Magnitude',\n",
    "        y=feature,\n",
    "        size='Insured Damage, Adjusted',\n",
    "        sizes=(20, 200),\n",
    "        hue='Insured Damage, Adjusted',\n",
    "        palette='viridis',\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"{feature}\", fontsize=18)\n",
    "    axes[i].set_ylabel(\"(US$)\", fontsize=18)  # Add 'US$' to y-axis label\n",
    "    axes[i].set_xlabel(\"Magnitude\", fontsize=18)  # Add label for x-axis and increase font size\n",
    "    axes[i].grid(True)\n",
    "    # Remove individual legends\n",
    "    scatter.legend_.remove()\n",
    "\n",
    "    # Increase tick size for both axes\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=18)  # Adjust the size for both axes\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add a shared color bar horizontally\n",
    "cbar_ax = fig.add_axes([0.1, -0.05, 0.75, 0.03])  # Lowered the color bar\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(\n",
    "    vmin=df_quakes_updated['Insured Damage, Adjusted'].min(),\n",
    "    vmax=df_quakes_updated['Insured Damage, Adjusted'].max()\n",
    "))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('Adjusted Insured Damage (US$)', fontsize=18)\n",
    "\n",
    "# Increase tick size for the color bar values\n",
    "cbar.ax.tick_params(labelsize=16)  # Increase the color bar tick values' font size\n",
    "\n",
    "# Flip x-axis for all subplots\n",
    "for ax in axes:\n",
    "    ax.invert_xaxis()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7a61d-256c-41af-9d05-dc23f496ddc6",
   "metadata": {},
   "source": [
    "# 5.0 Comparison with other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e999a2-a694-4a1b-9ec4-4aa9cf589c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_disaster_damage_heatmap(df):\n",
    "    \"\"\"\n",
    "    This function generates six heatmaps for each diaster type in EM-DAT focusing on the Conterminous United States.:\n",
    "    1. Total Damage\n",
    "    2. Insured Damage\n",
    "    3. CPI\n",
    "    4. No. Affected\n",
    "    5. No. Injured\n",
    "    6. Total Deaths\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing disaster data with 'Disaster Type', 'Country', \n",
    "                         'Latitude', 'Longitude', 'Total Damage, Adjusted', \n",
    "                         'Insured Damage, Adjusted', 'CPI', 'No. Affected',\n",
    "                         'No. Injured', 'Total Deaths', and 'Start Year'.\n",
    "    \"\"\"\n",
    "    # Define the boundaries for the Conterminous USA\n",
    "    lat_min, lat_max = 24.6, 50.0\n",
    "    lon_min, lon_max = -125.0, -65\n",
    "\n",
    "    # Filter the data for the Conterminous USA\n",
    "    df_conus = df[(df['Latitude'] >= lat_min) & \n",
    "                  (df['Latitude'] <= lat_max) & \n",
    "                  (df['Longitude'] >= lon_min) & \n",
    "                  (df['Longitude'] <= lon_max)].copy()\n",
    "\n",
    "    # Remove the specific earthquake in Mexico\n",
    "    df_conus = df_conus[df_conus['DisNo.'] != '1915-0004-USA']\n",
    "\n",
    "    # Ensure 'Start Year' column exists and use it to create 'Year'\n",
    "    if 'Start Year' in df_conus.columns:\n",
    "        df_conus['Year'] = df_conus['Start Year']\n",
    "    else:\n",
    "        raise ValueError(\"Column 'Start Year' not found in the DataFrame.\")\n",
    "    \n",
    "    # Get a list of all unique disaster types\n",
    "    disaster_types = df_conus['Disaster Type'].unique()\n",
    "\n",
    "    # Prepare a DataFrame to hold the aggregated data\n",
    "    heatmap_data = pd.DataFrame()\n",
    "\n",
    "    # Loop through each disaster type to gather data\n",
    "    for disaster in disaster_types:\n",
    "        df_disaster = df_conus[df_conus['Disaster Type'] == disaster]\n",
    "\n",
    "        # Group by year and sum variables\n",
    "        df_disaster_damage = df_disaster.groupby('Year')[\n",
    "            ['Total Damage, Adjusted', 'Insured Damage, Adjusted', 'No. Affected', 'No. Injured', 'Total Deaths']\n",
    "        ].sum()\n",
    "\n",
    "        # Scale values for Total Damage and Insured Damage (adjusting by 1000 due to reloading data in)\n",
    "        df_disaster_damage['Total Damage, Adjusted'] *= 1000 \n",
    "        df_disaster_damage['Insured Damage, Adjusted'] *= 1000\n",
    "\n",
    "        # If CPI exists, get the mean CPI per year per disaster type\n",
    "        if 'CPI' in df_disaster.columns:\n",
    "            df_disaster_damage['CPI'] = df_disaster.groupby('Year')['CPI'].mean()\n",
    "\n",
    "        # Add the disaster type to the data\n",
    "        df_disaster_damage['Disaster Type'] = disaster\n",
    "        df_disaster_damage = df_disaster_damage.reset_index()  # Ensure 'Year' remains in the DataFrame\n",
    "        heatmap_data = pd.concat([heatmap_data, df_disaster_damage], axis=0)\n",
    "\n",
    "\n",
    "    heatmaps = {\n",
    "        \"Total Damage (Adjusted)\": heatmap_data.pivot(index='Disaster Type', columns='Year', values='Total Damage, Adjusted'),\n",
    "        \"Insured Damage (Adjusted)\": heatmap_data.pivot(index='Disaster Type', columns='Year', values='Insured Damage, Adjusted'),\n",
    "        \"CPI\": heatmap_data.pivot(index='Disaster Type', columns='Year', values='CPI'),\n",
    "        \"No. Affected\": heatmap_data.pivot(index='Disaster Type', columns='Year', values='No. Affected'),\n",
    "        \"No. Injured\": heatmap_data.pivot(index='Disaster Type', columns='Year', values='No. Injured'),\n",
    "        \"Total Deaths\": heatmap_data.pivot(index='Disaster Type', columns='Year', values='Total Deaths')\n",
    "    }\n",
    "\n",
    "    damage_vmin, damage_vmax = 0, 3e10\n",
    "    affected_vmin, affected_vmax = 0, heatmap_data['No. Affected'].max()\n",
    "    injured_vmin, injured_vmax = 0, heatmap_data['No. Injured'].max()\n",
    "    deaths_vmin, deaths_vmax = 0, heatmap_data['Total Deaths'].max()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(35, 22))  # Two rows for six heatmaps\n",
    "\n",
    "    for ax, (title, data) in zip(axes.flatten(), heatmaps.items()):\n",
    "        if \"Total Damage\" in title or \"Insured Damage\" in title:\n",
    "            vmin, vmax = damage_vmin, damage_vmax\n",
    "        elif \"No. Affected\" in title:\n",
    "            vmin, vmax = affected_vmin, affected_vmax\n",
    "        elif \"No. Injured\" in title:\n",
    "            vmin, vmax = injured_vmin, injured_vmax\n",
    "        elif \"Total Deaths\" in title:\n",
    "            vmin, vmax = deaths_vmin, deaths_vmax\n",
    "        else:\n",
    "            vmin, vmax = None, None  # Default for CPI\n",
    "\n",
    "        sns.heatmap(data, annot=False, cmap=\"viridis\", fmt=\",.0f\" if vmin is not None else \",.2f\", linewidths=.5,\n",
    "                    cbar_kws={'label': 'Count' if 'No.' in title else 'US$ (Adjusted)' if 'Damage' in title else 'CPI'},\n",
    "                    ax=ax, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(title, fontsize=18)\n",
    "        ax.set_xlabel('Year', fontsize=18)\n",
    "        ax.set_ylabel('Disaster Type', fontsize=18)\n",
    "        ax.tick_params(axis='x', rotation=90, labelsize=18)\n",
    "        ax.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.ax.tick_params(labelsize=18)\n",
    "        cbar.set_label('Count' if 'No.' in title else 'US$ (Adjusted)' if 'Damage' in title else 'CPI', fontsize=20)  # Label font size\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_disaster_damage_heatmap(df_Geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2afde6-6029-4199-8696-372689ba38cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_violinplots(df, filename=\"violin_plots.png\"):\n",
    "    \"\"\"\n",
    "    This function generates horizontal violin plots for each relevant column, grouped by disaster type.\n",
    "    It shows min, median, max, and the distribution for each disaster type.\n",
    "    \"\"\"\n",
    "    # List of columns to plot with their respective units\n",
    "    cols = ['Total_Damage_Adjusted', 'Insured_Damage_Adjusted', 'No_Affected', 'No_Injured', 'Total_Deaths', \n",
    "            'CPI', 'Final_Adjusted_Loss', 'Expected_Loss', 'Excess_Loss']\n",
    "    \n",
    "    # Dictionary to hold units for each column\n",
    "    units = {\n",
    "        'Total_Damage_Adjusted': 'US$',\n",
    "        'Insured_Damage_Adjusted': 'US$',\n",
    "        'No_Affected': 'count',\n",
    "        'No_Injured': 'count',\n",
    "        'Total_Deaths': 'count',\n",
    "        'CPI': 'Index',\n",
    "        'Final_Adjusted_Loss': 'US$',\n",
    "        'Expected_Loss': 'US$',\n",
    "        'Excess_Loss': 'US$'\n",
    "    }\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(25, 18))  # 3x3 grid for 9 metrics\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Font sizes\n",
    "    title_fontsize = 18\n",
    "    label_fontsize = 18\n",
    "    tick_fontsize = 16\n",
    "    \n",
    "    for i, col in enumerate(cols):\n",
    "        # Create horizontal violin plot for each column grouped by 'Disaster Type'\n",
    "        sns.violinplot(y='Disaster Type', x=col, data=df, ax=axes[i], palette=\"muted\")\n",
    "        title = col.replace('_', ' ')\n",
    "        xlabel = f'{title} ({units[col]})'\n",
    "        \n",
    "        axes[i].set_title(f'{title}', fontsize=title_fontsize)\n",
    "        axes[i].set_ylabel('Disaster Type', fontsize=label_fontsize)\n",
    "        axes[i].set_xlabel(xlabel, fontsize=label_fontsize)\n",
    "        \n",
    "        axes[i].tick_params(axis='x', labelsize=tick_fontsize)\n",
    "        axes[i].tick_params(axis='y', labelsize=tick_fontsize)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_violinplots(T, filename=\"/Users/isabelsmith/Documents/axa/violin_plots.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1dfeb4-5f5e-4aab-8ea2-4b8dfeb70266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_table_by_disaster_type(df):\n",
    "    \"\"\"\n",
    "    Returns a table showing median for each numeric column in the DataFrame,\n",
    "    grouped by Disaster Type.\n",
    "    \"\"\"\n",
    "    cols = ['Total_Damage_Adjusted', 'Insured_Damage_Adjusted', 'No_Affected', 'No_Injured', 'Total_Deaths', \n",
    "            'CPI', 'Final_Adjusted_Loss', 'Expected_Loss', 'Excess_Loss']\n",
    "\n",
    "    summary = df.groupby('Disaster Type')[cols].agg([ 'median'])\n",
    "\n",
    "    summary.columns = [f'{col} ({stat})' for col, stat in summary.columns]\n",
    "\n",
    "    summary.index.name = 'Disaster Type'\n",
    "    summary.reset_index(inplace=True)\n",
    "    \n",
    "    return summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
